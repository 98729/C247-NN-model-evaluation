{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/MyDrive/247/project\"\n",
        "os.chdir(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kdO_r5B_XRL",
        "outputId": "38d67f58-a4c1-4f13-c6cd-0764c7feb390"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "m-Iz86_ZN-hn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(10, 6, 6)\n",
        "upsample = nn.ConvTranspose2d(10, 6, 3, stride=2, padding=1) # Cin = 10, Cout = 6\n",
        "output = upsample(input)\n",
        "print(output.size())\n",
        "layer = nn.ConvTranspose1d(6, 10, 3, stride=2, padding=1) # Cin = 6, Cout = 10\n",
        "output = layer(input)\n",
        "print(output.size())\n",
        "input = torch.randn((1, 100, 1))\n",
        "layer = nn.Sequential(\n",
        "    nn.ConvTranspose1d(100, 220, 4, 2, 0),\n",
        "    nn.BatchNorm1d(220),\n",
        "    nn.ReLU(True),\n",
        "    nn.ConvTranspose1d(220, 154, 4, 2, 0),\n",
        "    nn.BatchNorm1d(154),\n",
        "    nn.ReLU(True),\n",
        "    nn.ConvTranspose1d(154, 88, 4, 2, 0),\n",
        "    nn.BatchNorm1d(88),\n",
        "    nn.ReLU(True),\n",
        "    nn.ConvTranspose1d(88, 44, 7, 2, 0),\n",
        "    nn.BatchNorm1d(44),\n",
        "    nn.ReLU(True),\n",
        "    nn.ConvTranspose1d(44, 10, 4, 2, 0),\n",
        "    nn.ReLU(True),\n",
        "    # nn.LSTM(10, 22, 2, batch_first = True)\n",
        ")\n",
        "output = layer(input)\n",
        "print(output.size())\n",
        "output = output.transpose(2, 1)\n",
        "l = nn.LSTM(10, 22, 2, batch_first = True)\n",
        "output = l(output)\n",
        "print(len(output), len(output[0]), len(output[0][0]), len(output[0][0][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdjuLipw0YVU",
        "outputId": "8085eedf-f9ef-4c25-8f79-34dc7db82de6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 11, 11])\n",
            "torch.Size([10, 10, 11])\n",
            "torch.Size([1, 10, 100])\n",
            "2 1 100 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "rogNtN8W_QT5"
      },
      "outputs": [],
      "source": [
        "X_test = np.load(\"X_test.npy\")\n",
        "y_test = np.load(\"y_test.npy\")\n",
        "person_train_valid = np.load(\"person_train_valid.npy\")\n",
        "X_train_valid = np.load(\"X_train_valid.npy\")\n",
        "y_train_valid = np.load(\"y_train_valid.npy\")\n",
        "person_test = np.load(\"person_test.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAYIvgPw_QT7"
      },
      "source": [
        "### Shape of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDMLUmrS_QT9",
        "outputId": "a8310be0-00dd-470c-8e21-eb7441c5c9f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training/Valid data shape: (2115, 22, 1000)\n",
            "Test data shape: (443, 22, 1000)\n",
            "Training/Valid target shape: (2115,)\n",
            "Test target shape: (443,)\n",
            "Person train/valid shape: (2115, 1)\n",
            "Person test shape: (443, 1)\n"
          ]
        }
      ],
      "source": [
        "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
        "print ('Test data shape: {}'.format(X_test.shape))\n",
        "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
        "print ('Test target shape: {}'.format(y_test.shape))\n",
        "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
        "print ('Person test shape: {}'.format(person_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "  def __init__(self,input_dim, hidden_dims):\n",
        "    super(Generator,self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.layer = nn.Sequential(\n",
        "      nn.ConvTranspose1d(input_dim, hidden_dims[0], 4, 2, 0, bias = False),\n",
        "      nn.BatchNorm1d(hidden_dims[0]),\n",
        "      nn.ReLU(True),\n",
        "    \n",
        "      nn.ConvTranspose1d(hidden_dims[0], hidden_dims[1], 4, 2, 0, bias = False),\n",
        "      nn.BatchNorm1d(hidden_dims[1]),\n",
        "      nn.ReLU(True),\n",
        "\n",
        "      nn.ConvTranspose1d(hidden_dims[1], hidden_dims[2], 4, 2, 0, bias = False),\n",
        "      nn.BatchNorm1d(hidden_dims[2]),\n",
        "      nn.ReLU(True),\n",
        "\n",
        "      nn.ConvTranspose1d(hidden_dims[2], hidden_dims[3], 7, 2, 0, bias = False),\n",
        "      nn.BatchNorm1d(hidden_dims[3]),\n",
        "      nn.ReLU(True),\n",
        "\n",
        "      nn.ConvTranspose1d(hidden_dims[3], 22, 4, 2, 0, bias = False),\n",
        "      nn.ReLU(True),\n",
        "\n",
        "      # nn.LSTM(10, 22, 2, batch_first = True)\n",
        "    )\n",
        "\n",
        "  def forward(self,input):\n",
        "    return self.layer(input)\n"
      ],
      "metadata": {
        "id": "S_qd8wlWk0sv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  \"\"\"\n",
        "  Discriminator that uses CNN layers\n",
        "  follows the DCGAN of the pytorch tutorial\n",
        "  https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
        "  input is (batch, in_dim, seq)\n",
        "  output is (batch, 1, seq)\n",
        "  \"\"\"\n",
        "  def __init__(self, input_dim, hidden_dims):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.layer = nn.Sequential(\n",
        "      nn.Conv1d(input_dim, hidden_dims[0], 3, 2, 1, bias=False),\n",
        "      nn.LeakyReLU(0.2, inplace=True),\n",
        "    # output batch x dim1 x 50\n",
        "\n",
        "      nn.Conv1d(hidden_dims[0], hidden_dims[1], 3, 2, 1, bias=False),\n",
        "      nn.BatchNorm1d(hidden_dims[1]),\n",
        "      nn.LeakyReLU(0.2, inplace=True),\n",
        "    # ouptut batch x dim2 x 25\n",
        "\n",
        "      nn.Conv1d(hidden_dims[1], hidden_dims[2], 3, 2, 1, bias=False),\n",
        "      nn.BatchNorm1d(hidden_dims[2]),\n",
        "      nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "    # ouptut batch x dim3 x 13\n",
        "      nn.Conv1d(hidden_dims[2], hidden_dims[3], 3, 2, 1, bias=False),\n",
        "      nn.BatchNorm1d(hidden_dims[3]),\n",
        "      nn.LeakyReLU(0.2, inplace=True),\n",
        "    # ouptut batch x dim3 x 7\n",
        "\n",
        "      nn.Conv1d(hidden_dims[3], 1, 3, 2, 1, bias=False),\n",
        "      nn.BatchNorm1d(1),\n",
        "      nn.LeakyReLU(0.2, inplace=True),\n",
        "      nn.Conv1d(1, 1, 4, 1, 0, bias=False),\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)"
      ],
      "metadata": {
        "id": "ZSHGK1qdlApS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(object):\n",
        "  \"\"\"\n",
        "  Class object to control and abstract the training and logging of vanilla GANs\n",
        "  Assumes that the noise input or z vector is (batch_size, 2, 600) \n",
        "  where the first channel is noise and the second channel is the label\n",
        "  \"\"\"\n",
        "  def __init__(self, generator, discriminator, dataset, **kwargs):\n",
        "    self.generator = generator\n",
        "    self.discriminator = discriminator\n",
        "    self.loss = nn.BCELoss()\n",
        "    self.dataset = dataset\n",
        "    \n",
        "    # setting up optimizers\n",
        "    lr = kwargs.get('learn_rate', 0.0002)\n",
        "    w_decay = kwargs.get('weight_decay', 0.00001)\n",
        "    self.discriminator_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=lr, weight_decay=w_decay)\n",
        "    self.generator_optimizer = torch.optim.Adam(self.generator.parameters(), lr=lr, weight_decay=w_decay)\n",
        "\n",
        "\n",
        "  def train(self, epochs, batch_size, verbose=True, print_every=100):\n",
        "    \n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.generator_loss = list()\n",
        "    self.discriminator_loss = list()\n",
        "    self.generated_test = list()\n",
        "\n",
        "    # setup the loader\n",
        "    data_loader = torch.utils.data.DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
        "    iteration = 0\n",
        "\n",
        "    # TODO: change the input\n",
        "    fixed_z = torch.rand((1, self.generator.input_dim - 1, 1))\n",
        "    fixed_labels = torch.zeros((1 , 1)).reshape(-1, 1).repeat(1, 1).reshape(-1, 1, 1)\n",
        "    fixed_z = torch.cat([fixed_z, fixed_labels], dim=1)\n",
        "    real_label = 1\n",
        "    fake_label = 0\n",
        "    for epoch in range(self.epochs + 1):\n",
        "      for i, sample in enumerate(data_loader):\n",
        "        # What to do for each batch\n",
        "        if i == data_loader.dataset.__len__() // self.batch_size:\n",
        "          break\n",
        "\n",
        "        self.generator.train()\n",
        "\n",
        "        # assert data and labels\n",
        "        eeg_data = sample['data']\n",
        "        eeg_labels = sample['label']\n",
        "\n",
        "        # input vector for \n",
        "        noise = torch.rand((self.batch_size, self.generator.input_dim - 1, 1))\n",
        "        labels = eeg_labels.reshape(-1, 1).repeat(1, 1).reshape(-1, 1, 1)\n",
        "        \n",
        "        z = torch.cat([noise, labels], dim=1)\n",
        "\n",
        "        seq_length = eeg_data.shape[2]\n",
        "\n",
        "        benchmark = torch.full((self.batch_size, 1, 1), real_label, dtype=torch.float)\n",
        "\n",
        "       \n",
        "        eeg_data = Variable(eeg_data)\n",
        "        z = Variable(z)\n",
        "        \n",
        "\n",
        "        ##############\n",
        "        # Training the Discriminator\n",
        "        ##############\n",
        "        self.discriminator_optimizer.zero_grad()\n",
        "        self.discriminator.zero_grad() # remove previous gradients\n",
        "        # train the discriminator on real data\n",
        "        real_score = self.discriminator(eeg_data)\n",
        "        discriminator_loss_real = self.loss(real_score, benchmark)\n",
        "        discriminator_loss_real.backward()\n",
        "        real_output_score = real_score.mean().item()\n",
        "\n",
        "        # train the discriminator on fake data\n",
        "\n",
        "        benchmark.fill_(fake_label)\n",
        "        fake_data = self.generator(z)\n",
        "        fake_score = self.discriminator(fake_data.detach())\n",
        "        discriminator_loss_fake = self.loss(fake_score, benchmark)\n",
        "        discriminator_loss_fake.backward()\n",
        "        # print(fake_score)\n",
        "        G_output_score1 = fake_score.mean().item()\n",
        "        \n",
        "\n",
        "        # optimize the discriminator\n",
        "        discriminator_loss = discriminator_loss_real + discriminator_loss_fake\n",
        "        self.discriminator_optimizer.step()\n",
        "\n",
        "\n",
        "        ##############\n",
        "        # Training the Generator\n",
        "        ##############\n",
        "        self.generator.zero_grad() # remove previous gradients\n",
        "        self.generator_optimizer.zero_grad()\n",
        "\n",
        "        benchmark.fill_(real_label)\n",
        "        fake_score2 = self.discriminator(fake_data)\n",
        "        generator_output_score2 = fake_score2.mean().item()\n",
        "        generator_loss = self.loss(fake_score2, benchmark)\n",
        "        generator_loss.backward()\n",
        "        self.generator_optimizer.step()\n",
        "\n",
        "        iteration += 1\n",
        "        \n",
        "        if iteration % print_every == 0 and verbose:\n",
        "          # output the loss and the scores\n",
        "          print(\"Iteration : \", iteration)\n",
        "          print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tReal Score: %.4f\\t Fake Scores: %.4f / %.4f'\n",
        "                  % (epoch, self.epochs, i, len(data_loader),\n",
        "                     discriminator_loss.item(), generator_loss.item(), real_output_score, G_output_score1, generator_output_score2))\n",
        "          \n",
        "        self.generator_loss.append(generator_loss.cpu().item())\n",
        "        self.discriminator_loss.append(discriminator_loss.cpu().item())\n",
        "\n",
        "      # check how the Generator is doing\n",
        "      with torch.no_grad() :\n",
        "        self.generator.eval()\n",
        "        fake_data = self.generator(fixed_z).detach().cpu()\n",
        "        self.generated_test.append(np.mean(fake_data.numpy(), axis=(0, 1)))\n",
        "\n",
        "\n",
        "      # if epoch % 250 == 0:\n",
        "      #   save_str = save_path + '/Generator_' + str(epoch) + '.pth'\n",
        "      #   self.save_models(save_str)\n",
        "\n",
        "\n",
        "  def save_models(self, path):\n",
        "    torch.save(self.generator.state_dict(), path)\n",
        "    print(\"Saved generator at \" + path)\n",
        "    return True\n",
        "     "
      ],
      "metadata": {
        "id": "Kw3GnXWkPvUN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def moving_average(x, w):\n",
        "    \"\"\"\n",
        "    Simple 1d moving average using numpy\n",
        "    :input x: 1d np array\n",
        "    :input w: size of moving average filter\n",
        "    returns np array with applied moving average\n",
        "    \"\"\"\n",
        "    return np.convolve(x, np.ones(w), 'valid') / w\n",
        "\n",
        "class eegData():\n",
        "    \"\"\"\n",
        "    PyTorch style dataset to be loaded into torch data loader for training\n",
        "    provides simple functionality\n",
        "    \"\"\"\n",
        "    def __init__(self, data_file_name, label_file_name, device=torch.device('cpu'), preprocessing_params={}):\n",
        "        \"\"\"\n",
        "        :input data_file_name: file path of the data\n",
        "        :input label_file_name: file path of the labels\n",
        "        :input validation_size: size of validation (percentage given to validation)\n",
        "        :input device: specify if CUDA or GPU\n",
        "        :input preprocessing_params:\n",
        "            'subsample': int on the size of step of the subsampling\n",
        "            'mov_avg': int on the size of the moving average window\n",
        "            'trim': how many of the last indices will be trimmed off\n",
        "        \"\"\"\n",
        "        subsample = preprocessing_params.get('subsample', 1) # we can increase our trial count\n",
        "        mov_avg_window = preprocessing_params.get('mov_avg', 1) # limit ourselves to 2115\n",
        "        trimming = preprocessing_params.get('trim', 0) # how much you want to trim\n",
        "        eeg_data = np.load(data_file_name)\n",
        "        label_data = np.load(label_file_name) - 769\n",
        "        self.device = device\n",
        "        \n",
        "\n",
        "        # remove the last x amount of time steps\n",
        "        trimmed_indices = eeg_data.shape[2] - trimming \n",
        "        eeg_data = eeg_data[:, :, :trimmed_indices].copy()\n",
        "\n",
        "        #begin_subsample\n",
        "        stack_eeg_data = []\n",
        "        stack_label_data = []\n",
        "        for i in range(subsample):\n",
        "            sampled_eeg_data = eeg_data[:, :, i::subsample].copy()\n",
        "            stack_label_data.append(label_data.copy())\n",
        "            stack_eeg_data.append(sampled_eeg_data)\n",
        "\n",
        "        eeg_data = np.vstack(stack_eeg_data)\n",
        "        label_data = np.concatenate(stack_label_data)\n",
        "\n",
        "        # begin applying moving_average\n",
        "        eeg_data = np.apply_along_axis(func1d=moving_average, axis=2, arr=eeg_data, w=mov_avg_window)\n",
        "\n",
        "\n",
        "        self.eeg_data = torch.from_numpy(eeg_data).float().to(self.device)\n",
        "        self.label_data = torch.from_numpy(label_data).float().long().to(self.device)\n",
        "        self.mov_avg_window = mov_avg_window \n",
        "        self.trim = trimming \n",
        "        self.sampling = subsample\n",
        "\n",
        "    def __len__(self):\n",
        "        assert self.eeg_data.shape[0] == self.label_data.shape[0]\n",
        "        return self.eeg_data.shape[0]\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.numpy()\n",
        "\n",
        "        sample = {}\n",
        "        sample['data'] = self.eeg_data[idx]\n",
        "        sample['label'] = self.label_data[idx]\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "Bf5eaaeHCuwJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_prep(X,y,sub_sample,average,noise):\n",
        "    \n",
        "    total_X = None\n",
        "    total_y = None\n",
        "    \n",
        "    # Trimming the data (sample,22,1000) -> (sample,22,500)\n",
        "    X = X[:,:,0:500]\n",
        "    print('Shape of X after trimming:',X.shape)\n",
        "    \n",
        "    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n",
        "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
        "    \n",
        "    \n",
        "    total_X = X_max\n",
        "    total_y = y\n",
        "    print('Shape of X after maxpooling:',total_X.shape)\n",
        "    \n",
        "    # Averaging + noise \n",
        "    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average),axis=3)\n",
        "    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n",
        "    \n",
        "    total_X = np.vstack((total_X, X_average))\n",
        "    total_y = np.hstack((total_y, y))\n",
        "    print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n",
        "    \n",
        "    # Subsampling\n",
        "    \n",
        "    for i in range(sub_sample):\n",
        "        \n",
        "        X_subsample = X[:, :, i::sub_sample] + \\\n",
        "                            (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n",
        "            \n",
        "        total_X = np.vstack((total_X, X_subsample))\n",
        "        total_y = np.hstack((total_y, y))\n",
        "        \n",
        "    \n",
        "    print('Shape of X after subsampling and concatenating:',total_X.shape)\n",
        "    print('Shape of y after subsampling and concatenating:',total_y.shape)\n",
        "    return total_X,total_y"
      ],
      "metadata": {
        "id": "pA5ewW5LiJUP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Adjusting the labels so that \n",
        "\n",
        "# Cue onset left - 0\n",
        "# Cue onset right - 1\n",
        "# Cue onset foot - 2\n",
        "# Cue onset tongue - 3\n",
        "\n",
        "y_train_valid -= 769\n",
        "y_test -= 769\n",
        "\n",
        "\n",
        "## Random splitting and reshaping the data\n",
        "# First generating the training and validation indices using random splitting\n",
        "\n",
        "ind_valid = np.random.choice(2115, 375, replace=False)\n",
        "ind_train = np.array(list(set(range(2115)).difference(set(ind_valid))))\n",
        "\n",
        "# Creating the training and validation sets using the generated indices\n",
        "(X_train, X_valid) = X_train_valid[ind_train], X_train_valid[ind_valid] \n",
        "(y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
        "\n",
        "\n",
        "## Preprocessing the dataset\n",
        "x_train,y_train = data_prep(X_train,y_train,2,2,True)\n",
        "x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n",
        "X_test_prep,y_test_prep = data_prep(X_test,y_test,2,2,True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lsnEEbYnQ7a",
        "outputId": "4b85ae4d-c65f-46eb-c6d9-0bac998f441b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X after trimming: (1740, 22, 500)\n",
            "Shape of X after maxpooling: (1740, 22, 250)\n",
            "Shape of X after averaging+noise and concatenating: (3480, 22, 250)\n",
            "Shape of X after subsampling and concatenating: (6960, 22, 250)\n",
            "Shape of y after subsampling and concatenating: (6960,)\n",
            "Shape of X after trimming: (375, 22, 500)\n",
            "Shape of X after maxpooling: (375, 22, 250)\n",
            "Shape of X after averaging+noise and concatenating: (750, 22, 250)\n",
            "Shape of X after subsampling and concatenating: (1500, 22, 250)\n",
            "Shape of y after subsampling and concatenating: (1500,)\n",
            "Shape of X after trimming: (443, 22, 500)\n",
            "Shape of X after maxpooling: (443, 22, 250)\n",
            "Shape of X after averaging+noise and concatenating: (886, 22, 250)\n",
            "Shape of X after subsampling and concatenating: (1772, 22, 250)\n",
            "Shape of y after subsampling and concatenating: (1772,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch import Tensor\n",
        "\n",
        "testinp = torch.rand((1,99,1))\n",
        "test_label = torch.zeros((1,1,1))\n",
        "inp = torch.cat([testinp, test_label], dim = 1) # (1, 100, 1)\n",
        "generator = Generator(100, [220, 154, 88, 44])\n",
        "x = generator(inp)\n",
        "discriminator = Discriminator(22,[18, 14, 10, 6])\n",
        "dataset = eegData('X_train_valid.npy', 'y_train_valid.npy', preprocessing_params={'subsample':6, 'trim':400})\n",
        "gan = GAN(generator,discriminator,dataset, cuda = True)"
      ],
      "metadata": {
        "id": "Hj7WB8DJKLJX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gan.train(50,250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHP0xBCHKPYM",
        "outputId": "df582bfc-b8ba-4540-f36c-9a83786c0e86"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration :  100\n",
            "[1/50][49/51]\tLoss_D: 1.6890\tLoss_G: 0.4324\tReal Score: 0.5330\t Fake Scores: 0.6491 / 0.6489\n",
            "Iteration :  200\n",
            "[3/50][49/51]\tLoss_D: 1.6132\tLoss_G: 0.4481\tReal Score: 0.5568\t Fake Scores: 0.6389 / 0.6388\n",
            "Iteration :  300\n",
            "[5/50][49/51]\tLoss_D: 1.5529\tLoss_G: 0.4628\tReal Score: 0.5760\t Fake Scores: 0.6296 / 0.6295\n",
            "Iteration :  400\n",
            "[7/50][49/51]\tLoss_D: 1.5175\tLoss_G: 0.4767\tReal Score: 0.5819\t Fake Scores: 0.6209 / 0.6208\n",
            "Iteration :  500\n",
            "[9/50][49/51]\tLoss_D: 1.4986\tLoss_G: 0.4899\tReal Score: 0.5802\t Fake Scores: 0.6128 / 0.6127\n",
            "Iteration :  600\n",
            "[11/50][49/51]\tLoss_D: 1.4758\tLoss_G: 0.5023\tReal Score: 0.5811\t Fake Scores: 0.6052 / 0.6051\n",
            "Iteration :  700\n",
            "[13/50][49/51]\tLoss_D: 1.4631\tLoss_G: 0.5141\tReal Score: 0.5776\t Fake Scores: 0.5981 / 0.5980\n",
            "Iteration :  800\n",
            "[15/50][49/51]\tLoss_D: 1.4474\tLoss_G: 0.5254\tReal Score: 0.5767\t Fake Scores: 0.5914 / 0.5913\n",
            "Iteration :  900\n",
            "[17/50][49/51]\tLoss_D: 1.4412\tLoss_G: 0.5362\tReal Score: 0.5713\t Fake Scores: 0.5850 / 0.5850\n",
            "Iteration :  1000\n",
            "[19/50][49/51]\tLoss_D: 1.4339\tLoss_G: 0.5466\tReal Score: 0.5670\t Fake Scores: 0.5790 / 0.5789\n",
            "Iteration :  1100\n",
            "[21/50][49/51]\tLoss_D: 1.4257\tLoss_G: 0.5566\tReal Score: 0.5638\t Fake Scores: 0.5732 / 0.5732\n",
            "Iteration :  1200\n",
            "[23/50][49/51]\tLoss_D: 1.4241\tLoss_G: 0.5661\tReal Score: 0.5575\t Fake Scores: 0.5678 / 0.5677\n",
            "Iteration :  1300\n",
            "[25/50][49/51]\tLoss_D: 1.4161\tLoss_G: 0.5754\tReal Score: 0.5551\t Fake Scores: 0.5626 / 0.5625\n",
            "Iteration :  1400\n",
            "[27/50][49/51]\tLoss_D: 1.4132\tLoss_G: 0.5842\tReal Score: 0.5505\t Fake Scores: 0.5576 / 0.5575\n",
            "Iteration :  1500\n",
            "[29/50][49/51]\tLoss_D: 1.4085\tLoss_G: 0.5927\tReal Score: 0.5471\t Fake Scores: 0.5529 / 0.5528\n",
            "Iteration :  1600\n",
            "[31/50][49/51]\tLoss_D: 1.4063\tLoss_G: 0.6009\tReal Score: 0.5428\t Fake Scores: 0.5484 / 0.5483\n",
            "Iteration :  1700\n",
            "[33/50][49/51]\tLoss_D: 1.4028\tLoss_G: 0.6087\tReal Score: 0.5395\t Fake Scores: 0.5441 / 0.5441\n",
            "Iteration :  1800\n",
            "[35/50][49/51]\tLoss_D: 1.4014\tLoss_G: 0.6161\tReal Score: 0.5355\t Fake Scores: 0.5401 / 0.5400\n",
            "Iteration :  1900\n",
            "[37/50][49/51]\tLoss_D: 1.3989\tLoss_G: 0.6232\tReal Score: 0.5324\t Fake Scores: 0.5363 / 0.5362\n",
            "Iteration :  2000\n",
            "[39/50][49/51]\tLoss_D: 1.3969\tLoss_G: 0.6299\tReal Score: 0.5294\t Fake Scores: 0.5327 / 0.5326\n",
            "Iteration :  2100\n",
            "[41/50][49/51]\tLoss_D: 1.3945\tLoss_G: 0.6362\tReal Score: 0.5269\t Fake Scores: 0.5293 / 0.5293\n",
            "Iteration :  2200\n",
            "[43/50][49/51]\tLoss_D: 1.3936\tLoss_G: 0.6419\tReal Score: 0.5240\t Fake Scores: 0.5263 / 0.5263\n",
            "Iteration :  2300\n",
            "[45/50][49/51]\tLoss_D: 1.3931\tLoss_G: 0.6472\tReal Score: 0.5212\t Fake Scores: 0.5236 / 0.5235\n",
            "Iteration :  2400\n",
            "[47/50][49/51]\tLoss_D: 1.3917\tLoss_G: 0.6518\tReal Score: 0.5193\t Fake Scores: 0.5211 / 0.5211\n",
            "Iteration :  2500\n",
            "[49/50][49/51]\tLoss_D: 1.3915\tLoss_G: 0.6561\tReal Score: 0.5170\t Fake Scores: 0.5189 / 0.5189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo --quiet\n",
        "import torchinfo"
      ],
      "metadata": {
        "id": "oB60gwRbtY8G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "batch = 250\n",
        "Gtest = generator\n",
        "Gtest.to(device)\n",
        "print(torchinfo.summary(Gtest, input_size=(250, 100, 1)))\n",
        "Dtest = discriminator\n",
        "Dtest.to(device)\n",
        "print(torchinfo.summary(Dtest, input_size=(250, 22, 100)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qzskWYnsx-c",
        "outputId": "1b01c8ec-c2cf-48cf-9e60-4328c3092bd5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "Generator                                [250, 22, 100]            --\n",
            "├─Sequential: 1-1                        [250, 22, 100]            --\n",
            "│    └─ConvTranspose1d: 2-1              [250, 220, 4]             88,000\n",
            "│    └─BatchNorm1d: 2-2                  [250, 220, 4]             440\n",
            "│    └─ReLU: 2-3                         [250, 220, 4]             --\n",
            "│    └─ConvTranspose1d: 2-4              [250, 154, 10]            135,520\n",
            "│    └─BatchNorm1d: 2-5                  [250, 154, 10]            308\n",
            "│    └─ReLU: 2-6                         [250, 154, 10]            --\n",
            "│    └─ConvTranspose1d: 2-7              [250, 88, 22]             54,208\n",
            "│    └─BatchNorm1d: 2-8                  [250, 88, 22]             176\n",
            "│    └─ReLU: 2-9                         [250, 88, 22]             --\n",
            "│    └─ConvTranspose1d: 2-10             [250, 44, 49]             27,104\n",
            "│    └─BatchNorm1d: 2-11                 [250, 44, 49]             88\n",
            "│    └─ReLU: 2-12                        [250, 44, 49]             --\n",
            "│    └─ConvTranspose1d: 2-13             [250, 22, 100]            3,872\n",
            "│    └─ReLU: 2-14                        [250, 22, 100]            --\n",
            "==========================================================================================\n",
            "Total params: 309,716\n",
            "Trainable params: 309,716\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 1.15\n",
            "==========================================================================================\n",
            "Input size (MB): 0.10\n",
            "Forward/backward pass size (MB): 30.45\n",
            "Params size (MB): 1.24\n",
            "Estimated Total Size (MB): 31.79\n",
            "==========================================================================================\n",
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "Discriminator                            [250, 1, 1]               --\n",
            "├─Sequential: 1-1                        [250, 1, 1]               --\n",
            "│    └─Conv1d: 2-1                       [250, 18, 50]             1,188\n",
            "│    └─LeakyReLU: 2-2                    [250, 18, 50]             --\n",
            "│    └─Conv1d: 2-3                       [250, 14, 25]             756\n",
            "│    └─BatchNorm1d: 2-4                  [250, 14, 25]             28\n",
            "│    └─LeakyReLU: 2-5                    [250, 14, 25]             --\n",
            "│    └─Conv1d: 2-6                       [250, 10, 13]             420\n",
            "│    └─BatchNorm1d: 2-7                  [250, 10, 13]             20\n",
            "│    └─LeakyReLU: 2-8                    [250, 10, 13]             --\n",
            "│    └─Conv1d: 2-9                       [250, 6, 7]               180\n",
            "│    └─BatchNorm1d: 2-10                 [250, 6, 7]               12\n",
            "│    └─LeakyReLU: 2-11                   [250, 6, 7]               --\n",
            "│    └─Conv1d: 2-12                      [250, 1, 4]               18\n",
            "│    └─BatchNorm1d: 2-13                 [250, 1, 4]               2\n",
            "│    └─LeakyReLU: 2-14                   [250, 1, 4]               --\n",
            "│    └─Conv1d: 2-15                      [250, 1, 1]               4\n",
            "│    └─Sigmoid: 2-16                     [250, 1, 1]               --\n",
            "==========================================================================================\n",
            "Total params: 2,628\n",
            "Trainable params: 2,628\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 21.29\n",
            "==========================================================================================\n",
            "Input size (MB): 2.20\n",
            "Forward/backward pass size (MB): 3.91\n",
            "Params size (MB): 0.01\n",
            "Estimated Total Size (MB): 6.12\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Testing the model\n",
        "GAN_score = gan.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test accuracy of the hybrid CNN-LSTM model:', GAN_score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "LxIxUKDCzH4U",
        "outputId": "56936c3f-c48a-4a16-de46-963b9be2d870"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ea42b7e900b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Testing the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mGAN_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy of the hybrid CNN-LSTM model:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAN_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GAN' object has no attribute 'eval'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}